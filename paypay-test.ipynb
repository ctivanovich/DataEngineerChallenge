{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "074c04f1-de31-4025-91b5-755f90a89957",
   "metadata": {},
   "source": [
    "# PayPay Data Engineering Take-home Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "21a963ca-3854-47e7-8046-7a6c5e79f391",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql import SparkSession, Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "96a1f7cc-639b-4ddf-9299-8d704b80f55f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/Users/ct.ivanovich/opt/anaconda3/lib/python3.9/site-packages/pyspark/jars/spark-unsafe_2.12-3.2.1.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "22/06/23 15:29:34 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .master(\"local[*]\") # only for demo and testing purposes\n",
    "    .appName(\"Paypay Test\")\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d72bf296-a400-440d-b293-736d35816503",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [\n",
    "    \"timestamp\", \n",
    "    \"elb\",\n",
    "    \"client:port\",\n",
    "    \"backend:port\",\n",
    "    \"request_processing_time\",\n",
    "    \"backend_processing_time\",\n",
    "    \"response_processing_time\",\n",
    "    \"elb_status_code\",\n",
    "    \"backend_status_code\",\n",
    "    \"received_bytes\",\n",
    "    \"sent_bytes\",\n",
    "    \"request\", \n",
    "    \"user_agent\",\n",
    "    \"ssl_cipher\", \n",
    "    \"ssl_protocol\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ae493485-e0ed-4da7-b308-921fea54ad34",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df = (\n",
    "    spark.read.options(\n",
    "        compression = \"gzip\",\n",
    "        sep = \" \",\n",
    "        inferSchema = True\n",
    "    )\n",
    "    .csv(\"mktplace_shop_web_log_sample.log.gz\")\n",
    "    .toDF(*columns)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cb2d646b-ed8e-447f-b935-dbd4642f40b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- timestamp: string (nullable = true)\n",
      " |-- elb: string (nullable = true)\n",
      " |-- client:port: string (nullable = true)\n",
      " |-- backend:port: string (nullable = true)\n",
      " |-- request_processing_time: double (nullable = true)\n",
      " |-- backend_processing_time: double (nullable = true)\n",
      " |-- response_processing_time: double (nullable = true)\n",
      " |-- elb_status_code: integer (nullable = true)\n",
      " |-- backend_status_code: integer (nullable = true)\n",
      " |-- received_bytes: integer (nullable = true)\n",
      " |-- sent_bytes: integer (nullable = true)\n",
      " |-- request: string (nullable = true)\n",
      " |-- user_agent: string (nullable = true)\n",
      " |-- ssl_cipher: string (nullable = true)\n",
      " |-- ssl_protocol: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24f199df-538e-4585-800b-953f374947ae",
   "metadata": {},
   "source": [
    "## Processing & Analytical goals:\n",
    "---\n",
    "### Sessionize the web log by IP. Sessionize = aggregrate all page hits by visitor/IP during a session. https://en.wikipedia.org/wiki/Session_(web_analytics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "138c606e-2710-4e19-870c-54c34a27482d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = (\n",
    "    df\n",
    "    .withColumn(\"client:port\", split(df[\"client:port\"], \":\"))\n",
    "    .withColumn(\"client\", element_at(\"client:port\", 1))\n",
    "    .withColumn(\"backend:port\", split(df[\"backend:port\"], \":\"))\n",
    "    .withColumn(\"backend\", element_at(\"backend:port\", 1))\n",
    "    .withColumn(\"timestamp\", col(\"timestamp\").cast(\"timestamp\"))\n",
    "    .drop(*[\n",
    "        \"client:port\", \n",
    "        \"backend:port\",\n",
    "        \"request_processing_time\",\n",
    "        \"backend_processing_time\",\n",
    "        \"response_processing_time\",\n",
    "        \"elb_status_code\",\n",
    "        \"received_bytes\",\n",
    "        \"sent_bytes\",\n",
    "        \"request\", \n",
    "        \"user_agent\",\n",
    "        \"ssl_cipher\", \n",
    "        \"ssl_protocol\"\n",
    "    ])\n",
    "    .sort([\"client\", \"timestamp\"])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0b48b92-d638-42f0-af28-9c5b5b8d4e86",
   "metadata": {},
   "source": [
    "Restrict to status codes most likely to be associated with a valid request."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "25938774-9e4c-47d2-85e1-994e4013ac2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.filter(\"backend_status_code >= 200 and backend_status_code < 400\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f1feacaf-b211-415d-9402-7fce4c254bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get last event timestamp\n",
    "df = df.withColumn(\n",
    "    \"last_event\", \n",
    "    lag(\"timestamp\", 1).over(Window.orderBy(\"timestamp\").partitionBy(\"client\"))\n",
    ")\n",
    "\n",
    "# get the length by taking difference b/t this event and last event\n",
    "df = df.withColumn(\n",
    "    \"event_duration\",\n",
    "    when(df[\"last_event\"].isNull(), 0).otherwise(unix_timestamp(df[\"timestamp\"]) - unix_timestamp(df[\"last_event\"]))\n",
    ")\n",
    "\n",
    "# if last event is null, it's the first event for that session, or if the difference is longer than our timeout period of 15 min\n",
    "df = df.withColumn(\n",
    "    \"is_new_session\",\n",
    "    when(\n",
    "        df[\"last_event\"].isNull(), 1\n",
    "    )\n",
    "    .when(\n",
    "        df[\"event_duration\"] >= 60*15, 1\n",
    "    )\n",
    "    .otherwise(0)\n",
    ")\n",
    "\n",
    "# assign session ID by summing yes/no of new_session? events\n",
    "df = df.withColumn(\n",
    "    \"user_session_id\",\n",
    "    sum(\"is_new_session\").over(Window.orderBy(\"timestamp\").partitionBy(\"client\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d4ea5e05-9256-4493-8a34-9856461b1194",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView(\"sessions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be55bc6b-9804-41a6-a7a2-63174f60e312",
   "metadata": {},
   "source": [
    "### Determine the average session time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2d4fbdf0-04ed-4c3d-ac25-0f47c0686251",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark.sql(\n",
    "    \"\"\"\n",
    "    SELECT \n",
    "        client, \n",
    "        AVG(session_length) AS average_session_length\n",
    "    FROM (\n",
    "        SELECT\n",
    "            client,\n",
    "            user_session_id,\n",
    "            SUM(event_duration) AS session_length\n",
    "        FROM sessions\n",
    "        GROUP BY 1, 2\n",
    "    ) t1\n",
    "    GROUP BY 1\n",
    "    \"\"\"\n",
    ").repartition(1).write.csv(\"average_session_length.csv\", header=True, mode = \"overwrite\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60bef774-90d9-47b5-83e7-e754890d3c5f",
   "metadata": {},
   "source": [
    "### Determine unique URL visits per session. To clarify, count a hit to a unique URL only once per session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c35eff21-f6a3-469f-b4da-1ef5a706bec3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        client, \n",
    "        user_session_id, \n",
    "        COUNT(DISTINCT backend) AS unique_urls_per_session\n",
    "    FROM sessions\n",
    "    GROUP BY 1, 2\n",
    "\"\"\").repartition(1).write.csv(\"unique_visits_per_session.csv\", header=True, mode = \"overwrite\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a34f89e-2e07-4c22-9c25-371da0c714a6",
   "metadata": {},
   "source": [
    "### Find the most engaged users, ie the IPs with the longest session times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "94b2d98a-b3eb-4adc-a91d-c1aa1040668e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        client,\n",
    "        SUM(session_length) AS total_sessions_length\n",
    "     FROM (\n",
    "        SELECT\n",
    "            client,\n",
    "            user_session_id,\n",
    "            SUM(event_duration) AS session_length\n",
    "        FROM sessions\n",
    "        GROUP BY 1, 2\n",
    "    ) t1\n",
    "    GROUP BY 1\n",
    "    ORDER BY 2 DESC\n",
    "    LIMIT 100\n",
    "\"\"\").repartition(1).write.csv(\"100_most_engaged_users.csv\", header=True, mode = \"overwrite\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
